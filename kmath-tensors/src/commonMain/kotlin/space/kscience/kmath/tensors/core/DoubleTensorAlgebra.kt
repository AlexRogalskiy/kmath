/*
 * Copyright 2018-2022 KMath contributors.
 * Use of this source code is governed by the Apache 2.0 license that can be found in the license/LICENSE.txt file.
 */


@file:OptIn(PerformancePitfall::class)

package space.kscience.kmath.tensors.core

import space.kscience.kmath.PerformancePitfall
import space.kscience.kmath.linear.transpose
import space.kscience.kmath.nd.*
import space.kscience.kmath.operations.DoubleBufferOps
import space.kscience.kmath.operations.DoubleField
import space.kscience.kmath.structures.*
import space.kscience.kmath.tensors.api.AnalyticTensorAlgebra
import space.kscience.kmath.tensors.api.LinearOpsTensorAlgebra
import space.kscience.kmath.tensors.api.Tensor
import space.kscience.kmath.tensors.core.internal.*
import kotlin.math.*
import kotlin.reflect.KFunction3

/**
 * Implementation of basic operations over double tensors and basic algebra operations on them.
 */
@OptIn(PerformancePitfall::class)
public open class DoubleTensorAlgebra :
    AnalyticTensorAlgebra<Double, DoubleField>,
    LinearOpsTensorAlgebra<Double, DoubleField> {

    public companion object : DoubleTensorAlgebra()

    override val elementAlgebra: DoubleField get() = DoubleField

    public val bufferAlgebra: DoubleBufferOps get() = DoubleBufferOps


    /**
     * Applies the [transform] function to each element of the tensor and returns the resulting modified tensor.
     *
     * @param transform the function to be applied to each element of the tensor.
     * @return the resulting tensor after applying the function.
     */
    @Suppress("OVERRIDE_BY_INLINE")
    final override inline fun StructureND<Double>.map(transform: DoubleField.(Double) -> Double): DoubleTensor {
        val tensor = asDoubleTensor()
        //TODO remove additional copy
        val array = DoubleBuffer(tensor.source.size) { DoubleField.transform(tensor.source[it]) }
        return DoubleTensor(
            tensor.shape,
            array,
        )
    }

    public inline fun Tensor<Double>.mapInPlace(operation: (Double) -> Double) {
        if (this is DoubleTensor) {
            source.mapInPlace(operation)
        } else {
            indices.forEach { set(it, operation(get(it))) }
        }
    }

    public inline fun Tensor<Double>.mapIndexedInPlace(operation: DoubleField.(IntArray, Double) -> Double) {
        indices.forEach { set(it, DoubleField.operation(it, get(it))) }
    }

    @Suppress("OVERRIDE_BY_INLINE")
    final override inline fun StructureND<Double>.mapIndexed(transform: DoubleField.(index: IntArray, Double) -> Double): DoubleTensor {
        return copyToTensor().apply { mapIndexedInPlace(transform) }
    }


    @Suppress("OVERRIDE_BY_INLINE")
    final override inline fun zip(
        left: StructureND<Double>,
        right: StructureND<Double>,
        transform: DoubleField.(Double, Double) -> Double,
    ): DoubleTensor {
        checkShapesCompatible(left, right)

        val leftTensor = left.asDoubleTensor()
        val rightTensor = right.asDoubleTensor()
        val buffer = DoubleBuffer(leftTensor.source.size) {
            DoubleField.transform(leftTensor.source[it], rightTensor.source[it])
        }
        return DoubleTensor(leftTensor.shape, buffer)
    }


    public inline fun StructureND<Double>.reduceElements(transform: (DoubleBuffer) -> Double): Double =
        transform(asDoubleTensor().source.copy())
    //TODO Add read-only DoubleBuffer wrapper. To avoid protective copy

    override fun StructureND<Double>.valueOrNull(): Double? {
        val dt = asDoubleTensor()
        return if (dt.shape contentEquals ShapeND(1)) dt.source[0] else null
    }

    override fun StructureND<Double>.value(): Double = valueOrNull()
        ?: throw IllegalArgumentException("The tensor shape is $shape, but value method is allowed only for shape [1]")

    public fun fromBuffer(shape: ShapeND, buffer: Buffer<Double>): DoubleTensor {
        checkNotEmptyShape(shape)
        check(buffer.size > 0) { "Illegal empty buffer provided" }
        check(buffer.size == shape.linearSize) {
            "Inconsistent shape $shape for buffer of size ${buffer.size} provided"
        }
        return DoubleTensor(shape, buffer.toDoubleBuffer())
    }


    /**
     * Constructs a tensor with the specified shape and data.
     *
     * @param shape the desired shape for the tensor.
     * @param array one-dimensional data array.
     * @return tensor with the [shape] shape and [array] data.
     */
    public fun fromArray(shape: ShapeND, array: DoubleArray): DoubleTensor = fromBuffer(shape, array.asBuffer())

    /**
     * Constructs a tensor with the specified shape and initializer.
     *
     * @param shape the desired shape for the tensor.
     * @param initializer mapping tensor indices to values.
     * @return tensor with the [shape] shape and data generated by the [initializer].
     */
    override fun structureND(shape: ShapeND, initializer: DoubleField.(IntArray) -> Double): DoubleTensor = fromArray(
        shape,
        RowStrides(shape).asSequence().map { DoubleField.initializer(it) }.toMutableList().toDoubleArray()
    )

    override fun Tensor<Double>.getTensor(i: Int): DoubleTensor {
        val dt = asDoubleTensor()
        val lastShape = shape.last(shape.size - 1)
        val newShape: ShapeND = if (lastShape.isNotEmpty()) lastShape else ShapeND(1)
        return DoubleTensor(
            newShape,
            dt.source.view(newShape.linearSize * i, newShape.linearSize)
        )
    }

    /**
     * Creates a tensor of a given shape and fills all elements with a given value.
     *
     * @param value the value to fill the output tensor with.
     * @param shape array of integers defining the shape of the output tensor.
     * @return tensor with the [shape] shape and filled with [value].
     */
    public fun full(value: Double, shape: ShapeND): DoubleTensor {
        checkNotEmptyShape(shape)
        val buffer = DoubleBuffer(shape.linearSize) { value }
        return DoubleTensor(shape, buffer)
    }

    /**
     * Returns a tensor with the same shape as `input` filled with [value].
     *
     * @param value the value to fill the output tensor with.
     * @return tensor with the `input` tensor shape and filled with [value].
     */
    public fun fullLike(structureND: StructureND<*>, value: Double): DoubleTensor {
        val shape = structureND.shape
        val buffer = DoubleBuffer(structureND.indices.linearSize) { value }
        return DoubleTensor(shape, buffer)
    }

    /**
     * Returns a tensor filled with the scalar value `0.0`, with the shape defined by the variable argument [shape].
     *
     * @param shape array of integers defining the shape of the output tensor.
     * @return tensor filled with the scalar value `0.0`, with the [shape] shape.
     */
    public fun zeros(shape: ShapeND): DoubleTensor = full(0.0, shape)

    /**
     * Returns a tensor filled with the scalar value `0.0`, with the same shape as a given array.
     *
     * @return tensor filled with the scalar value `0.0`, with the same shape as `input` tensor.
     */
    public fun zeroesLike(structureND: StructureND<*>): DoubleTensor = fullLike(structureND, 0.0)

    /**
     * Returns a tensor filled with the scalar value `1.0`, with the shape defined by the variable argument [shape].
     *
     * @param shape array of integers defining the shape of the output tensor.
     * @return tensor filled with the scalar value `1.0`, with the [shape] shape.
     */
    public fun ones(shape: ShapeND): DoubleTensor = full(1.0, shape)

    /**
     * Returns a tensor filled with the scalar value `1.0`, with the same shape as a given array.
     *
     * @return tensor filled with the scalar value `1.0`, with the same shape as `input` tensor.
     */
    public fun onesLike(structureND: StructureND<*>): DoubleTensor = fullLike(structureND, 1.0)

    /**
     * Returns a 2D tensor with shape ([n], [n]), with ones on the diagonal and zeros elsewhere.
     *
     * @param n the number of rows and columns
     * @return a 2-D tensor with ones on the diagonal and zeros elsewhere.
     */
    public fun eye(n: Int): DoubleTensor {
        val shape = ShapeND(n, n)
        val buffer = DoubleBuffer(n * n) { 0.0 }
        val res = DoubleTensor(shape, buffer)
        for (i in 0 until n) {
            res[intArrayOf(i, i)] = 1.0
        }
        return res
    }

    override fun Double.plus(arg: StructureND<Double>): DoubleTensor = arg.map { this@plus + it }

    override fun StructureND<Double>.plus(arg: Double): DoubleTensor = map { it + arg }

    override fun StructureND<Double>.plus(arg: StructureND<Double>): DoubleTensor = zip(this, arg) { l, r -> l + r }

    override fun Tensor<Double>.plusAssign(value: Double) {
        mapInPlace { it + value }
    }

    override fun Tensor<Double>.plusAssign(arg: StructureND<Double>) {
        checkShapesCompatible(asDoubleTensor(), arg.asDoubleTensor())
        mapIndexedInPlace { index, value ->
            value + arg[index]
        }
    }

    override fun Double.minus(arg: StructureND<Double>): DoubleTensor = arg.map { this@minus - it }

    override fun StructureND<Double>.minus(arg: Double): DoubleTensor = map { it - arg }

    override fun StructureND<Double>.minus(arg: StructureND<Double>): DoubleTensor = zip(this, arg) { l, r -> l - r }

    override fun Tensor<Double>.minusAssign(value: Double) {
        mapInPlace { it - value }
    }

    override fun Tensor<Double>.minusAssign(arg: StructureND<Double>) {
        checkShapesCompatible(this, arg)
        mapIndexedInPlace { index, value -> value - arg.getDouble(index) }
    }

    override fun Double.times(arg: StructureND<Double>): DoubleTensor = arg.map { this@times * it }

    override fun StructureND<Double>.times(arg: Double): DoubleTensor = arg * asDoubleTensor()

    override fun StructureND<Double>.times(arg: StructureND<Double>): DoubleTensor = zip(this, arg) { l, r -> l * r }

    override fun Tensor<Double>.timesAssign(value: Double) {
        mapInPlace { it * value }
    }

    override fun Tensor<Double>.timesAssign(arg: StructureND<Double>) {
        checkShapesCompatible(this, arg)
        mapIndexedInPlace { index, value -> value * arg[index] }
    }

    override fun Double.div(arg: StructureND<Double>): DoubleTensor = arg.map { this@div / it }

    override fun StructureND<Double>.div(arg: Double): DoubleTensor = map { it / arg }

    override fun StructureND<Double>.div(arg: StructureND<Double>): DoubleTensor = zip(this, arg) { l, r -> l / r }

    override fun Tensor<Double>.divAssign(value: Double) {
        mapInPlace { it / value }
    }

    override fun Tensor<Double>.divAssign(arg: StructureND<Double>) {
        checkShapesCompatible(asDoubleTensor(), arg)
        mapIndexedInPlace { index, value -> value / arg[index] }
    }

    override fun StructureND<Double>.unaryMinus(): DoubleTensor = map { -it }

    override fun StructureND<Double>.transposed(i: Int, j: Int): Tensor<Double> {
        val actualI = if (i >= 0) i else shape.size + i
        val actualJ = if (j >= 0) j else shape.size + j
        return asDoubleTensor().permute(
            shape.transposed(actualI, actualJ)
        ) { originIndex ->
            originIndex.copyOf().apply {
                val ith = get(actualI)
                val jth = get(actualJ)
                set(actualI, jth)
                set(actualJ, ith)
            }
        }
//        // TODO change strides instead of changing content
//        val dt = asDoubleTensor()
//        val ii = dt.minusIndex(i)
//        val jj = dt.minusIndex(j)
//        checkTranspose(dt.dimension, ii, jj)
//        val n = dt.linearSize
//        val resBuffer = DoubleArray(n)
//
//        val resShape = dt.shape.copyOf()
//        resShape[ii] = resShape[jj].also { resShape[jj] = resShape[ii] }
//
//        val resTensor = DoubleTensor(resShape, resBuffer.asBuffer())
//
//        for (offset in 0 until n) {
//            val oldMultiIndex = dt.indices.index(offset)
//            val newMultiIndex = oldMultiIndex.copyOf()
//            newMultiIndex[ii] = newMultiIndex[jj].also { newMultiIndex[jj] = newMultiIndex[ii] }
//
//            val linearIndex = resTensor.indices.offset(newMultiIndex)
//            resTensor.source[linearIndex] = dt.source[offset]
//        }
//        return resTensor
    }

    override fun Tensor<Double>.view(shape: ShapeND): DoubleTensor {
        checkView(asDoubleTensor(), shape)
        return DoubleTensor(shape, asDoubleTensor().source)
    }

    override fun Tensor<Double>.viewAs(other: StructureND<Double>): DoubleTensor =
        view(other.shape)

    /**
     * Broadcasting Matrix product of two tensors.
     *
     * The behavior depends on the dimensionality of the tensors as follows:
     * 1. If both tensors are 1-dimensional, the dot product (scalar) is returned.
     *
     * 2. If both arguments are 2-dimensional, the matrix-matrix product is returned.
     *
     * 3. If the first argument is 1-dimensional and the second argument is 2-dimensional,
     * a 1 is prepended to its dimension for the purpose of the matrix multiply.
     * After the matrix multiply, depending on the implementation the prepended dimension might be removed.
     *
     * 4. If the first argument is 2-dimensional and the second argument is 1-dimensional,
     * the matrix-vector product is returned.
     *
     * 5. If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2),
     * then a batched matrix multiply is returned. If the first argument is 1-dimensional,
     * a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after.
     * If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix
     * multiple and removed after.
     * The non-matrix (i.e., batch) dimensions are broadcast (and thus must be broadcastable).
     * For example, if `input` is a (j &times; 1 &times; n &times; n) tensor and `other` is a
     * (k &times; n &times; n) tensor, out will be a (j &times; k &times; n &times; n) tensor.
     *
     * For more information: https://pytorch.org/docs/stable/generated/torch.matmul.html
     *
     * @param other tensor to be multiplied.
     * @return a mathematical product of two tensors.
     */
    public infix fun StructureND<Double>.matmul(other: StructureND<Double>): DoubleTensor {
        if (shape.size == 1 && other.shape.size == 1) {
            return DoubleTensor(ShapeND(1), DoubleBuffer(times(other).sum()))
        }

        var penultimateDim = false
        var lastDim = false

        //TODO do we need protective copy here?
        var newThis: DoubleTensor = copyToTensor()
        var newOther: DoubleTensor = other.copyToTensor()

        if (shape.size == 1) {
            penultimateDim = true
            newThis = newThis.view(ShapeND(1) + shape)
        }

        if (other.shape.size == 1) {
            lastDim = true
            newOther = newOther.view(other.shape + intArrayOf(1))
        }

        val broadcastTensors = broadcastOuterTensors(newThis, newOther)
        newThis = broadcastTensors[0]
        newOther = broadcastTensors[1]

        val l = newThis.shape[newThis.shape.size - 2]
        val m1 = newThis.shape[newThis.shape.size - 1]
        val m2 = newOther.shape[newOther.shape.size - 2]
        val n = newOther.shape[newOther.shape.size - 1]
        check(m1 == m2) {
            "Tensors dot operation dimension mismatch: ($l, $m1) x ($m2, $n)"
        }

        val resShape = newThis.shape.slice(0..(newThis.shape.size - 2)) + intArrayOf(newOther.shape.last())
        val resSize = resShape.linearSize
        val resTensor = DoubleTensor(resShape, DoubleArray(resSize).asBuffer())

        val resMatrices = resTensor.matrices
        val newThisMatrices = newThis.matrices
        val newOtherMatrices = newOther.matrices

        for (i in resMatrices.indices) {
            dotTo(newThisMatrices[i], newOtherMatrices[i], resMatrices[i], l, m1, n)
        }
//
//        for ((res, ab) in resTensor.matrixSequence().zip(newThis.matrixSequence().zip(newOther.matrixSequence()))) {
//            val (a, b) = ab
//            dotTo(a, b, res, l, m1, n)
//        }

        return if (penultimateDim) {
            resTensor.view(resTensor.shape.first(resTensor.shape.size - 2) + ShapeND(resTensor.shape.last()))
        } else if (lastDim) {
            resTensor.view(resTensor.shape.first(resTensor.shape.size - 1))
        } else {
            resTensor
        }
    }

    override fun StructureND<Double>.dot(other: StructureND<Double>): DoubleTensor {
        return if (dimension in 0..2 && other.dimension in 0..2) this.matmul(other)
        else error("Only vectors and matrices are allowed in non-broadcasting dot operation")
    }

    override fun diagonalEmbedding(
        diagonalEntries: StructureND<Double>,
        offset: Int,
        dim1: Int,
        dim2: Int,
    ): DoubleTensor {
        val n = diagonalEntries.shape.size
        val d1 = minusIndexFrom(n + 1, dim1)
        val d2 = minusIndexFrom(n + 1, dim2)

        check(d1 != d2) {
            "Diagonal dimensions cannot be identical $d1, $d2"
        }
        check(d1 <= n && d2 <= n) {
            "Dimension out of range"
        }

        var lessDim = d1
        var greaterDim = d2
        var realOffset = offset
        if (lessDim > greaterDim) {
            realOffset *= -1
            lessDim = greaterDim.also { greaterDim = lessDim }
        }

        val resShape = diagonalEntries.shape.slice(0 until lessDim) +
                intArrayOf(diagonalEntries.shape[n - 1] + abs(realOffset)) +
                diagonalEntries.shape.slice(lessDim until greaterDim - 1) +
                intArrayOf(diagonalEntries.shape[n - 1] + abs(realOffset)) +
                diagonalEntries.shape.slice(greaterDim - 1 until n - 1)
        val resTensor: DoubleTensor = zeros(resShape)

        for (i in 0 until diagonalEntries.indices.linearSize) {
            val multiIndex = diagonalEntries.indices.index(i)

            var offset1 = 0
            var offset2 = abs(realOffset)
            if (realOffset < 0) {
                offset1 = offset2.also { offset2 = offset1 }
            }
            val diagonalMultiIndex = multiIndex.slice(0 until lessDim).toIntArray() +
                    intArrayOf(multiIndex[n - 1] + offset1) +
                    multiIndex.slice(lessDim until greaterDim - 1).toIntArray() +
                    intArrayOf(multiIndex[n - 1] + offset2) +
                    multiIndex.slice(greaterDim - 1 until n - 1).toIntArray()

            resTensor[diagonalMultiIndex] = diagonalEntries[multiIndex]
        }

        return resTensor
    }

    /**
     * Compares element-wise two tensors with a specified precision.
     *
     * @param other the tensor to compare with `input` tensor.
     * @param epsilon permissible error when comparing two Double values.
     * @return true if two tensors have the same shape and elements, false otherwise.
     */
    public fun Tensor<Double>.eq(other: Tensor<Double>, epsilon: Double): Boolean =
        asDoubleTensor().eq(other) { x, y -> abs(x - y) < epsilon }

    /**
     * Compares element-wise two tensors.
     * Comparison of two Double values occurs with `1e-5` precision.
     *
     * @param other the tensor to compare with `input` tensor.
     * @return true if two tensors have the same shape and elements, false otherwise.
     */
    public infix fun Tensor<Double>.eq(other: Tensor<Double>): Boolean = eq(other, 1e-5)

    private fun Tensor<Double>.eq(
        other: Tensor<Double>,
        eqFunction: (Double, Double) -> Boolean,
    ): Boolean {
        //TODO optimize tensor conversion
        checkShapesCompatible(asDoubleTensor(), other)
        val n = asDoubleTensor().linearSize
        if (n != other.asDoubleTensor().linearSize) {
            return false
        }
        for (i in 0 until n) {
            if (!eqFunction(asDoubleTensor().source[i], other.asDoubleTensor().source[i])) {
                return false
            }
        }
        return true
    }

    /**
     * Builds tensor from rows of the input tensor.
     *
     * @param indices the [IntArray] of 1-dimensional indices
     * @return tensor with rows corresponding to row by [indices]
     */
    public fun Tensor<Double>.rowsByIndices(indices: IntArray): DoubleTensor = stack(indices.map { getTensor(it) })


    private inline fun StructureND<Double>.foldDimToDouble(
        dim: Int,
        keepDim: Boolean,
        foldFunction: (DoubleArray) -> Double,
    ): DoubleTensor {
        check(dim < dimension) { "Dimension $dim out of range $dimension" }
        val resShape = if (keepDim) {
            shape.first(dim) + intArrayOf(1) + shape.last(dimension - dim - 1)
        } else {
            shape.first(dim) + shape.last(dimension - dim - 1)
        }
        val resNumElements = resShape.linearSize
        val init = foldFunction(DoubleArray(1) { 0.0 })
        val resTensor = DoubleTensor(
            resShape,
            DoubleBuffer(resNumElements) { init }
        )
        val dt = asDoubleTensor()
        for (index in resTensor.indices) {
            val prefix = index.take(dim).toIntArray()
            val suffix = index.takeLast(dimension - dim - 1).toIntArray()
            resTensor[index] = foldFunction(DoubleArray(shape[dim]) { i ->
                dt[prefix + intArrayOf(i) + suffix]
            })
        }
        return resTensor
    }

    private inline fun StructureND<Double>.foldDimToInt(
        dim: Int,
        keepDim: Boolean,
        foldFunction: (DoubleArray) -> Int,
    ): IntTensor {
        check(dim < dimension) { "Dimension $dim out of range $dimension" }
        val resShape = if (keepDim) {
            shape.first(dim) + intArrayOf(1) + shape.last(dimension - dim - 1)
        } else {
            shape.first(dim) + shape.last(dimension - dim - 1)
        }
        val resNumElements = resShape.linearSize
        val init = foldFunction(DoubleArray(1) { 0.0 })
        val resTensor = IntTensor(
            resShape,
            IntBuffer(resNumElements) { init }
        )
        for (index in resTensor.indices) {
            val prefix = index.take(dim).toIntArray()
            val suffix = index.takeLast(dimension - dim - 1).toIntArray()
            resTensor[index] = foldFunction(DoubleArray(shape[dim]) { i ->
                asDoubleTensor()[prefix + intArrayOf(i) + suffix]
            })
        }
        return resTensor
    }


    override fun StructureND<Double>.sum(): Double = reduceElements { it.array.sum() }

    override fun StructureND<Double>.sum(dim: Int, keepDim: Boolean): DoubleTensor =
        foldDimToDouble(dim, keepDim) { x -> x.sum() }

    override fun StructureND<Double>.min(): Double = reduceElements { it.array.min() }

    override fun StructureND<Double>.min(dim: Int, keepDim: Boolean): DoubleTensor =
        foldDimToDouble(dim, keepDim) { x -> x.minOrNull()!! }

    override fun StructureND<Double>.argMin(dim: Int, keepDim: Boolean): Tensor<Int> = foldDimToInt(dim, keepDim) { x ->
        x.withIndex().minBy { it.value }.index
    }

    override fun StructureND<Double>.max(): Double = reduceElements { it.array.max() }

    override fun StructureND<Double>.max(dim: Int, keepDim: Boolean): DoubleTensor =
        foldDimToDouble(dim, keepDim) { x -> x.maxOrNull()!! }


    override fun StructureND<Double>.argMax(dim: Int, keepDim: Boolean): IntTensor =
        foldDimToInt(dim, keepDim) { x ->
            x.withIndex().maxBy { it.value }.index
        }


    override fun mean(structureND: StructureND<Double>): Double = structureND.sum() / structureND.indices.linearSize

    override fun mean(structureND: StructureND<Double>, dim: Int, keepDim: Boolean): Tensor<Double> =
        structureND.foldDimToDouble(dim, keepDim) { arr ->
            check(dim < structureND.dimension) { "Dimension $dim out of range ${structureND.dimension}" }
            arr.sum() / structureND.shape[dim]
        }

    override fun std(structureND: StructureND<Double>): Double = structureND.reduceElements { arr ->
        val mean = arr.array.sum() / structureND.indices.linearSize
        sqrt(arr.array.sumOf { (it - mean) * (it - mean) } / (structureND.indices.linearSize - 1))
    }

    override fun std(structureND: StructureND<Double>, dim: Int, keepDim: Boolean): Tensor<Double> =
        structureND.foldDimToDouble(
            dim,
            keepDim
        ) { arr ->
            check(dim < structureND.dimension) { "Dimension $dim out of range ${structureND.dimension}" }
            val mean = arr.sum() / structureND.shape[dim]
            sqrt(arr.sumOf { (it - mean) * (it - mean) } / (structureND.shape[dim] - 1))
        }

    override fun variance(structureND: StructureND<Double>): Double = structureND.reduceElements { arr ->
        val linearSize = structureND.indices.linearSize
        val mean = arr.array.sum() / linearSize
        arr.array.sumOf { (it - mean) * (it - mean) } / (linearSize - 1)
    }

    override fun variance(structureND: StructureND<Double>, dim: Int, keepDim: Boolean): Tensor<Double> =
        structureND.foldDimToDouble(
            dim,
            keepDim
        ) { arr ->
            check(dim < structureND.dimension) { "Dimension $dim out of range ${structureND.dimension}" }
            val mean = arr.sum() / structureND.shape[dim]
            arr.sumOf { (it - mean) * (it - mean) } / (structureND.shape[dim] - 1)
        }


    override fun exp(arg: StructureND<Double>): DoubleTensor = arg.map { this.exp(it) }

    override fun ln(arg: StructureND<Double>): DoubleTensor = arg.map { this.ln(it) }

    override fun sqrt(arg: StructureND<Double>): DoubleTensor = arg.map { this.sqrt(it) }

    override fun cos(arg: StructureND<Double>): DoubleTensor = arg.map { this.cos(it) }

    override fun acos(arg: StructureND<Double>): DoubleTensor = arg.map { this.acos(it) }

    override fun cosh(arg: StructureND<Double>): DoubleTensor = arg.map { this.cosh(it) }

    override fun acosh(arg: StructureND<Double>): DoubleTensor = arg.map { this.acosh(it) }

    override fun sin(arg: StructureND<Double>): DoubleTensor = arg.map { this.sin(it) }

    override fun asin(arg: StructureND<Double>): DoubleTensor = arg.map { this.asin(it) }

    override fun sinh(arg: StructureND<Double>): DoubleTensor = arg.map { this.sinh(it) }

    override fun asinh(arg: StructureND<Double>): DoubleTensor = arg.map { this.asinh(it) }

    override fun tan(arg: StructureND<Double>): DoubleTensor = arg.map { this.tan(it) }

    override fun atan(arg: StructureND<Double>): DoubleTensor = arg.map { this.atan(it) }

    override fun tanh(arg: StructureND<Double>): DoubleTensor = arg.map { this.tanh(it) }

    override fun atanh(arg: StructureND<Double>): DoubleTensor = arg.map { this.atanh(it) }

    override fun power(arg: StructureND<Double>, pow: Number): StructureND<Double> = if (pow is Int) {
        arg.map { it.pow(pow) }
    } else {
        arg.map { it.pow(pow.toDouble()) }
    }

    override fun ceil(arg: StructureND<Double>): DoubleTensor = arg.map { ceil(it) }

    override fun floor(structureND: StructureND<Double>): DoubleTensor = structureND.map { floor(it) }

    override fun StructureND<Double>.inv(): DoubleTensor = invLU(this, 1e-9)

    override fun StructureND<Double>.det(): DoubleTensor = detLU(this, 1e-9)

    override fun lu(structureND: StructureND<Double>): Triple<DoubleTensor, DoubleTensor, DoubleTensor> =
        lu(structureND, 1e-9)

    override fun cholesky(structureND: StructureND<Double>): DoubleTensor = cholesky(structureND, 1e-6)

    override fun qr(structureND: StructureND<Double>): Pair<DoubleTensor, DoubleTensor> {
        checkSquareMatrix(structureND.shape)
        val qTensor = zeroesLike(structureND)
        val rTensor = zeroesLike(structureND)

        //TODO replace with cycle
        structureND.asDoubleTensor().matrixSequence()
            .zip(
                (qTensor.matrixSequence()
                    .zip(rTensor.matrixSequence()))
            ).forEach { (matrix, qr) ->
                val (q, r) = qr
                qrHelper(matrix, q, r.asDoubleTensor2D())
            }

        return qTensor to rTensor
    }

    override fun svd(
        structureND: StructureND<Double>,
    ): Triple<StructureND<Double>, StructureND<Double>, StructureND<Double>> =
        svdGolubKahan(structureND = structureND, epsilon = 1e-10)

    override fun symEig(structureND: StructureND<Double>): Pair<DoubleTensor, DoubleTensor> =
        symEigJacobi(structureND = structureND, maxIteration = 50, epsilon = 1e-15)

    override fun solve(a: MutableStructure2D<Double>, b: MutableStructure2D<Double>): MutableStructure2D<Double> {
        val aSvd = DoubleTensorAlgebra.svd(a)
        val s = BroadcastDoubleTensorAlgebra.diagonalEmbedding(aSvd.second.map {1.0 / it})
        val aInverse = aSvd.third.dot(s).dot(aSvd.first.transposed())
        return aInverse.dot(b).as2D()
    }

    override fun lm(
        func: KFunction3<MutableStructure2D<Double>, MutableStructure2D<Double>, LMSettings, MutableStructure2D<Double>>,
        p_input: MutableStructure2D<Double>, t_input: MutableStructure2D<Double>, y_dat_input: MutableStructure2D<Double>,
        weight_input: MutableStructure2D<Double>, dp_input: MutableStructure2D<Double>, p_min_input: MutableStructure2D<Double>, p_max_input: MutableStructure2D<Double>,
        c_input: MutableStructure2D<Double>, opts_input: DoubleArray, nargin: Int, example_number: Int): Double {

        var result_chi_sq = 0.0

        val tensor_parameter = 0
        val eps:Double = 2.2204e-16

        var settings = LMSettings(0, 0, example_number)
        settings.func_calls = 0 // running count of function evaluations

        var p = p_input
        val y_dat = y_dat_input
        val t = t_input

        val Npar   = length(p)                                    // number of parameters
        val Npnt   = length(y_dat)                                // number of data points
        var p_old = zeros(ShapeND(intArrayOf(Npar, 1))).as2D()    // previous set of parameters
        var y_old = zeros(ShapeND(intArrayOf(Npnt, 1))).as2D()    // previous model, y_old = y_hat(t;p_old)
        var X2 = 1e-3 / eps                                       // a really big initial Chi-sq value
        var X2_old = 1e-3 / eps                                   // a really big initial Chi-sq value
        var J = zeros(ShapeND(intArrayOf(Npnt, Npar))).as2D()     // Jacobian matrix
        val DoF = Npnt - Npar + 1                                 // statistical degrees of freedom

        var corr_p = 0
        var sigma_p = 0
        var sigma_y = 0
        var R_sq = 0
        var cvg_hist = 0

        if (length(t) != length(y_dat)) {
            println("lm.m error: the length of t must equal the length of y_dat")
            val length_t = length(t)
            val length_y_dat = length(y_dat)
            X2 = 0.0

            corr_p = 0
            sigma_p = 0
            sigma_y = 0
            R_sq = 0
            cvg_hist = 0

//            if (tensor_parameter != 0) { // Зачем эта проверка?
//                return
//            }
        }

        var weight = weight_input
        if (nargin <  5) {
            weight = fromArray(ShapeND(intArrayOf(1, 1)), doubleArrayOf((y_dat.transpose().dot(y_dat)).as1D()[0])).as2D()
        }

        var dp = dp_input
        if (nargin < 6) {
            dp = fromArray(ShapeND(intArrayOf(1, 1)), doubleArrayOf(0.001)).as2D()
        }

        var p_min = p_min_input
        if (nargin < 7) {
            p_min = p
            p_min.abs()
            p_min = p_min.div(-100.0).as2D()
        }

        var p_max = p_max_input
        if (nargin < 8) {
            p_max = p
            p_max.abs()
            p_max = p_max.div(100.0).as2D()
        }

        var c = c_input
        if (nargin < 9) {
            c = fromArray(ShapeND(intArrayOf(1, 1)), doubleArrayOf(1.0)).as2D()
        }

        var opts = opts_input
        if (nargin < 10) {
            opts = doubleArrayOf(3.0, 10.0 * Npar, 1e-3, 1e-3, 1e-1, 1e-1, 1e-2, 11.0, 9.0, 1.0)
        }

        val prnt          = opts[0]                // >1 intermediate results; >2 plots
        val MaxIter       = opts[1].toInt()        // maximum number of iterations
        val epsilon_1     = opts[2]                // convergence tolerance for gradient
        val epsilon_2     = opts[3]                // convergence tolerance for parameters
        val epsilon_3     = opts[4]                // convergence tolerance for Chi-square
        val epsilon_4     = opts[5]                // determines acceptance of a L-M step
        val lambda_0      = opts[6]                // initial value of damping paramter, lambda
        val lambda_UP_fac = opts[7]                // factor for increasing lambda
        val lambda_DN_fac = opts[8]                // factor for decreasing lambda
        val Update_Type   = opts[9].toInt()        // 1: Levenberg-Marquardt lambda update
        // 2: Quadratic update
        // 3: Nielsen's lambda update equations

        val plotcmd = "figure(102); plot(t(:,1),y_init,''-k'',t(:,1),y_hat,''-b'',t(:,1),y_dat,''o'',''color'',[0,0.6,0],''MarkerSize'',4); title(sprintf(''\\chi^2_\\nu = %f'',X2/DoF)); drawnow"

        p_min = make_column(p_min)
        p_max = make_column(p_max)

        if (length(make_column(dp)) == 1) {
            dp = ones(ShapeND(intArrayOf(Npar, 1))).div(1 / dp[0, 0]).as2D()
        }

        val idx = get_zero_indices(dp)                 // indices of the parameters to be fit
        val Nfit = idx?.shape?.component1()            // number of parameters to fit
        var stop = false                               // termination flag
        val y_init = feval(func, t, p, settings)       // residual error using p_try

        if (weight.shape.component1() == 1 || variance(weight) == 0.0) { // identical weights vector
            weight = ones(ShapeND(intArrayOf(Npnt, 1))).div(1 / kotlin.math.abs(weight[0, 0])).as2D() // !!! need to check
            println("using uniform weights for error analysis")
        }
        else {
            weight = make_column(weight)
            weight.abs()
        }

        // initialize Jacobian with finite difference calculation
        var lm_matx_ans = lm_matx(func, t, p_old, y_old,1, J, p, y_dat, weight, dp, settings)
        var JtWJ = lm_matx_ans[0]
        var JtWdy = lm_matx_ans[1]
        X2 = lm_matx_ans[2][0, 0]
        var y_hat = lm_matx_ans[3]
        J = lm_matx_ans[4]

        if ( abs(JtWdy).max()!! < epsilon_1 ) {
            println(" *** Your Initial Guess is Extremely Close to Optimal ***\n")
            println(" *** epsilon_1 = %e\n$epsilon_1")
            stop = true
        }

        var lambda = 1.0
        var nu = 1
        when (Update_Type) {
            1 -> lambda  = lambda_0                       // Marquardt: init'l lambda
            else -> {                                     // Quadratic and Nielsen
                lambda  = lambda_0 * (diag(JtWJ)).max()!!
                nu = 2
            }
        }

        X2_old = X2 // previous value of X2
        var cvg_hst = ones(ShapeND(intArrayOf(MaxIter, Npar + 3)))         // initialize convergence history

        var h = JtWJ.copyToTensor()
        var dX2 = X2
        while (!stop && settings.iteration <= MaxIter) {                   //--- Start Main Loop
            settings.iteration += 1

            // incremental change in parameters
            h = when (Update_Type) {
                1 -> {                // Marquardt
                    val solve = solve(JtWJ.plus(make_matrx_with_diagonal(diag(JtWJ)).div(1 / lambda)).as2D(), JtWdy)
                    solve.asDoubleTensor()
                }

                else -> {             // Quadratic and Nielsen
                    val solve = solve(JtWJ.plus(lm_eye(Npar).div(1 / lambda)).as2D(), JtWdy)
                    solve.asDoubleTensor()
                }
            }

            // big = max(abs(h./p)) > 2;                      % this is a big step

            // --- Are parameters [p+h] much better than [p] ?

            var p_try = (p + h).as2D()  // update the [idx] elements
            p_try = smallest_element_comparison(largest_element_comparison(p_min, p_try.as2D()), p_max)  // apply constraints

            var delta_y = y_dat.minus(feval(func, t, p_try, settings))   // residual error using p_try

            // TODO
            //if ~all(isfinite(delta_y))                     // floating point error; break
            //    stop = 1;
            //    break
            //end

            settings.func_calls += 1

            val tmp = delta_y.times(weight)
            var X2_try = delta_y.as2D().transpose().dot(tmp)     // Chi-squared error criteria

            val alpha = 1.0
            if (Update_Type == 2) { // Quadratic
                // One step of quadratic line update in the h direction for minimum X2

//            TODO
//            val alpha =  JtWdy.transpose().dot(h)  / ((X2_try.minus(X2)).div(2.0).plus(2 * JtWdy.transpose().dot(h)))
//            alpha =  JtWdy'*h / ( (X2_try - X2)/2 + 2*JtWdy'*h ) ;
//            h = alpha * h;
//
//            p_try = p + h(idx);                          % update only [idx] elements
//            p_try = min(max(p_min,p_try),p_max);         % apply constraints
//
//            delta_y = y_dat - feval(func,t,p_try,c);     % residual error using p_try
//            func_calls = func_calls + 1;
//            тX2_try = delta_y' * ( delta_y .* weight );   % Chi-squared error criteria
            }

            val rho = when (Update_Type) { // Nielsen
                1 -> {
                    val tmp = h.transposed().dot(make_matrx_with_diagonal(diag(JtWJ)).div(1 / lambda).dot(h).plus(JtWdy))
                    X2.minus(X2_try).as2D()[0, 0] / abs(tmp.as2D()).as2D()[0, 0]
                }
                else -> {
                    val tmp = h.transposed().dot(h.div(1 / lambda).plus(JtWdy))
                    X2.minus(X2_try).as2D()[0, 0] / abs(tmp.as2D()).as2D()[0, 0]
                }
            }

            println()
            println("rho = " + rho)

            if (rho > epsilon_4) { // it IS significantly better
                val dX2 = X2.minus(X2_old)
                X2_old = X2
                p_old = p.copyToTensor().as2D()
                y_old = y_hat.copyToTensor().as2D()
                p = make_column(p_try) // accept p_try

                lm_matx_ans = lm_matx(func, t, p_old, y_old, dX2.toInt(), J, p, y_dat, weight, dp, settings)
                // decrease lambda ==> Gauss-Newton method

                JtWJ = lm_matx_ans[0]
                JtWdy = lm_matx_ans[1]
                X2 = lm_matx_ans[2][0, 0]
                y_hat = lm_matx_ans[3]
                J = lm_matx_ans[4]

                lambda = when (Update_Type) {
                    1 -> { // Levenberg
                        max(lambda / lambda_DN_fac, 1e-7);
                    }
                    2 -> { // Quadratic
                        max( lambda / (1 + alpha) , 1e-7 );
                    }
                    else -> { // Nielsen
                        nu = 2
                        lambda * max( 1.0 / 3, 1 - (2 * rho - 1).pow(3) )
                    }
                }

                // if (prnt > 2) {
                //    eval(plotcmd)
                // }
            }
            else { // it IS NOT better
                X2 = X2_old // do not accept p_try
                if (settings.iteration % (2 * Npar) == 0 ) { // rank-1 update of Jacobian
                    lm_matx_ans = lm_matx(func, t, p_old, y_old,-1, J, p, y_dat, weight, dp, settings)
                    JtWJ = lm_matx_ans[0]
                    JtWdy = lm_matx_ans[1]
                    dX2 = lm_matx_ans[2][0, 0]
                    y_hat = lm_matx_ans[3]
                    J = lm_matx_ans[4]
                }

                // increase lambda  ==> gradient descent method
                lambda = when (Update_Type) {
                    1 -> { // Levenberg
                        min(lambda * lambda_UP_fac, 1e7)
                    }
                    2 -> { // Quadratic
                        lambda + abs(((X2_try.as2D()[0, 0] - X2) / 2) / alpha)
                    }
                    else -> { // Nielsen
                        nu *= 2
                        lambda * (nu / 2)
                    }
                }
            }

            if (prnt > 1) {
                val chi_sq = X2 / DoF
                println("Iteration $settings.iteration, func_calls $settings.func_calls | chi_sq=$chi_sq | lambda=$lambda")
                print("param: ")
                for (pn in 0 until Npar) {
                    print(p[pn, 0].toString() + " ")
                }
                print("\ndp/p: ")
                for (pn in 0 until Npar) {
                    print((h.as2D()[pn, 0] / p[pn, 0]).toString() + " ")
                }
                result_chi_sq = chi_sq
            }

            // update convergence history ... save _reduced_ Chi-square
            // cvg_hst(iteration,:) = [ func_calls  p'  X2/DoF lambda ];

            if (abs(JtWdy).max()!! < epsilon_1 && settings.iteration > 2) {
                println(" **** Convergence in r.h.s. (\"JtWdy\")  ****")
                println(" **** epsilon_1 = $epsilon_1")
                stop = true
            }
            if ((abs(h.as2D()).div(abs(p) + 1e-12)).max() < epsilon_2  &&  settings.iteration > 2) {
                println(" **** Convergence in Parameters ****")
                println(" **** epsilon_2 = $epsilon_2")
                stop = true
            }
            if (X2 / DoF < epsilon_3 && settings.iteration > 2) {
                println(" **** Convergence in reduced Chi-square  **** ")
                println(" **** epsilon_3 = $epsilon_3")
                stop = true
            }
            if (settings.iteration == MaxIter) {
                println(" !! Maximum Number of Iterations Reached Without Convergence !!")
                stop = true
            }
        }  // --- End of Main Loop

        // --- convergence achieved, find covariance and confidence intervals

        // ---- Error Analysis ----

//    if (weight.shape.component1() == 1 || weight.variance() == 0.0) {
//        weight = DoF / (delta_y.transpose().dot(delta_y)) * ones(intArrayOf(Npt, 1))
//    }

//    if (nargout > 1) {
//        val redX2 = X2 / DoF
//    }
//
//    lm_matx_ans = lm_matx(func, t, p_old, y_old, -1, J, p, y_dat, weight, dp)
//    JtWJ = lm_matx_ans[0]
//    JtWdy = lm_matx_ans[1]
//    X2 = lm_matx_ans[2][0, 0]
//    y_hat = lm_matx_ans[3]
//    J = lm_matx_ans[4]
//
//    if (nargout > 2) { // standard error of parameters
//        covar_p = inv(JtWJ);
//        siif nagma_p = sqrt(diag(covar_p));
//    }
//
//    if (nargout > 3) { // standard error of the fit
//        ///  sigma_y = sqrt(diag(J * covar_p * J'));        // slower version of below
//        sigma_y = zeros(Npnt,1);
//        for i=1:Npnt
//            sigma_y(i) = J(i,:) * covar_p * J(i,:)';
//        end
//        sigma_y = sqrt(sigma_y);
//    }
//
//    if (nargout > 4) { // parameter correlation matrix
//        corr_p = covar_p ./ [sigma_p*sigma_p'];
//    }
//
//    if (nargout > 5) { // coefficient of multiple determination
//        R_sq = corr([y_dat y_hat]);
//        R_sq = R_sq(1,2).^2;
//    }
//
//    if (nargout > 6) { // convergence history
//        cvg_hst = cvg_hst(1:iteration,:);
//    }

        return result_chi_sq
    }
}

public val Double.Companion.tensorAlgebra: DoubleTensorAlgebra get() = DoubleTensorAlgebra
public val DoubleField.tensorAlgebra: DoubleTensorAlgebra get() = DoubleTensorAlgebra


